# CMPS 2200  Recitation 02

**Name (Team Member 1):**Raymond Liu_________________________  
**Name (Team Member 2):**_________________________

In this recitation, we will investigate recurrences. 
To complete this recitation, follow the instructions in this document. Some of your answers will go in this file, and others will require you to edit `main.py`.



## Running and testing your code
- To run tests, from the command-line shell, you can run
  + `pytest test_main.py` will run all tests
  + `pytest test_main.py::test_one` will just run `test_one`
  + We recommend running one test at a time as you are debugging.

## Turning in your work

- Once complete, click on the "Git" icon in the left pane on repl.it.
- Enter a commit message in the "what did you change?" text box
- Click "commit and push." This will push your code to your github repository.
- Although you are working as a team, please have each team member submit the same code to their repository. One person can copy the code to their repl.it and submit it from there.

## Recurrences

In class, we've started looking at recurrences and how to we can establish asymptotic bounds on their values as a function of $n$. In this lab, we'll write some code to generate recursion trees (via a recursive function) for certain kinds of recurrences. By summing up nodes in the recurrence tree (that represent contributions to the recurrence) we can compare their total cost against the corresponding asymptotic bounds. We'll focus on  recurrences of the form:

$$ W(n) = aW(n/b) + f(n) $$

where $W(1) = 1$.

- [ ] 1. (2 point) In `main.py`, you have stub code which includes a function `simple_work_calc`. Implement this function to return the value of $W(n)$ for arbitrary values of $a$ and $b$ with $f(n)=n$.

- [ ] 2. (2 point) Test that your function is correct by calling from the command-line `pytest test_main.py::test_simple_work` by completing the test cases and adding 3 additional ones.

- [ ] 3. (2 point) Now implement `work_calc`, which generalizes the above so that we can now input $a$, $b$ and a *function* $f(n)$ as arguments. Test this code by completing the test cases in `test_work` and adding 3 more cases.

- [ ] 4. (2 point) Now, derive the asymptotic behavior of $W(n)$ using $f(n) = 1$, $f(n) = \log n$ and $f(n) = n$. Then, generate actual values for $W(n)$ for your code and confirm that the trends match your derivations.

**TODO: 
For f(n) = 1: The work done at each level of recursion is constant, regardless of the problem size. This implies that the total work is primarily influenced by the depth of the recursion tree. For a binary recursion (a=b=2), the depth of the tree is logb_n, resulting in a total work of O(n) because the work at each level is doubled, but the number of levels is logarithmic.
W(n)=O(n)

For f(n)=logn: The work at each recursive call grows logarithmically with the size of the problem. This additional work adds up over the levels of recursion. In the case of binary recursion, the total work would combine the logarithmic increase due to f(n) at each level with the exponential growth of the number of calls. This results in: W(n)=O(nlogn)
This is because, at each level, the total work due to f(n) is n, and there are logb_n levels.

For f(n)=n: The work at each level of recursion is linear in relation to the size of the problem at that level. This means that at each recursive step, the work grows linearly with n, and when combined with the recursive call structure, results in a total work of: W(n)=O(n^logb_a)

- [ ] 5. (4 points) Now that you have a nice way to empirically generate valuess of $W(n)$, we can look at the relationship between $a$, $b$, and $f(n)$. Suppose that $f(n) = n^c$. What is the asypmptotic behavior of $W(n)$ if $c < \log_b a$? What about $c > \log_b a$? And if they are equal? Modify `test_compare_work` to compare empirical values for different work functions (at several different values of $n$) to justify your answer. 

**TODO:
For $c < \log_b a$: a: The work done at each recursive call, f(n)=n^c
 , grows slower than the number of recursive calls, which is increasing exponentially at a rate determined by a and b. The Master Theorem tells us that in this case, the asymptotic behavior of W(n)  is dominated by the recursive calls, resulting in W(n) = O(n^logb_a)
For $c > \log_b a$: The work at each level of the recursion, f(n), grows faster than the number of subproblems generated by the recursion. The larger the value of c compared to logb_a, the more the work at each subproblem level dominates. Hence, the total work is primarily determined by the computation at each level rather than the number of recursive calls, leading to W(n) = O(n^c)
For $c = \log_b a$: When the rate of work growth fn = n^c matches the rate at which the number of subproblems grows, c = logb_a, both the recursive work and the work at each level contribute equally to the total work. In this situation, the Master Theorem indicates that the total work is the product of the number of levels and the work per level, yielding W(n)=Î˜(n^c logn)


- [ ] 6. (3 points) $W(n)$ is meant to represent the running time of some recursive algorithm. Suppose we always had $a$ processors available to us and we wanted to compute the span of the same algorithm. Implement the function `span_calc` to compute the empirical span, where the work of the algorithm is given by $W(n)$. Implement `test_compare_span` to create a new comparison function for comparing span functions. Derive the asymptotic expressions for the span of the recurrences you used in problem 4 above. Confirm that everything matches up as it should. 

**TODO:
For  f(n)=1: The span S(n) is dictated by the depth of the recursion tree since the work at each level is constant. Thus, for binary recursion (a=b=2), the asymptotic span is O(logn), reflecting the parallel execution's ability to handle each level's work concurrently.

For f(n)=logn: Each level adds logarithmic work, but since all recursive calls at a level can proceed in parallel, the dominant factor in the span is still the recursion depth. Therefore, the asymptotic span remains O(logn), albeit with a slightly higher constant due to the increased work at each level.

For f(n)=n: The linear work at each level suggests a more significant impact on the span since it increases with n. However, the span's growth rate is tempered by the parallel execution. Assuming a=b=2, the critical path through the algorithm is determined by the depth of the recursion plus the most significant work done at any level, leading to an asymptotic span of O(n), considering the linear work cannot be fully parallelized.